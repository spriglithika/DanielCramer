{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fec88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot, animation\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from utils import plotting_utils\n",
    "from utils import lidar_utils\n",
    "from importlib import reload\n",
    "reload(plotting_utils)\n",
    "reload(lidar_utils)\n",
    "from tqdm import tqdm\n",
    "from utils.simulation import BotSimulation, plot_predicted_map\n",
    "from IPython.display import Image, display\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple rectangular map with two circles in the top left and bottom right corners and a wall in the middle that goes halfway up the height of the room\n",
    "map_size = 40\n",
    "robot_size = map_size/10\n",
    "room = plotting_utils.get_map_objects(map_size)\n",
    "\n",
    "fig, ax = plotting_utils.generate_map(room,map_size)\n",
    "\n",
    "# set the aspect ratio to be equal\n",
    "pyplot.axis('equal')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c781f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "room = plotting_utils.get_map_objects(map_size)\n",
    "# Print the room objects to verify\n",
    "print(\"Walls:\")\n",
    "for wall in room['walls']:\n",
    "    print(f\"  {wall}\")\n",
    "print(\"Circles:\")\n",
    "for circle in room['circles']:\n",
    "    print(f\"  Center: {circle['center']}, Radius: {circle['radius']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in an x and y coordinate, a radius,\n",
    "# and a size for the robot and lidar, and returns the coordinates\n",
    "# of the individual lines that make up the robot and lidar.\n",
    "robot,lidar = plotting_utils.create_robot(8, 8, 25, size=robot_size)\n",
    "print(\"Robot Octagon Coordinates:\", robot)\n",
    "print(\"Lidar Line Coordinates:\", lidar)\n",
    "# We need a matplotlib axis to plot the room, robot, and lidar\n",
    "# We use the cordinate version of the room for this (wall and circle objects)\n",
    "room = plotting_utils.get_map_objects(map_size)\n",
    "fig, ax = pyplot.subplots()\n",
    "plotting_utils.show_room_and_robot(ax, room, robot,lidar)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf60c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example path for the robot to follow\n",
    "path_orientations = [\n",
    "    (10, 10, 45), (20, 20, 25), (30, 30, 15), (40, 25, 45),\n",
    "    (50, 10, 25), (60, 20, 57), (70, 30, 180), (70, 35, 170)\n",
    "]\n",
    "# Here we animate the robot movement along the path\n",
    "room = plotting_utils.get_map_objects(map_size)  # Get the room objects again\n",
    "fig, ax = plotting_utils.generate_map(room,size=map_size)  # Use the previously defined map generation function\n",
    "ax.set_xlim(0, map_size * 2)\n",
    "ax.set_ylim(0, map_size)\n",
    "ax.set_aspect('equal')\n",
    "anim = plotting_utils.animate_robot_movement(ax, path_orientations, size=robot_size)\n",
    "# Save the animation as a GIF\n",
    "anim.save('basic_robot_animation.gif', fps=2)\n",
    "# Show the animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7baad",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Now that we have visualised the basic maps and we have an idea of what we are looking at, we need to start simulating some lidar data. \n",
    "\n",
    "**Complete the missing code for the 4 blocks in lidar data following the instructions in the comments.**\n",
    "\n",
    "Feel free to ask questions, this is quite math heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_lidar_rays(lidar, room, num_rays=10, ray_length=100):\n",
    "    lidar_x, lidar_y = lidar[0]  # Starting point of the lidar\n",
    "    front_x, front_y = lidar[1]  # Front point of the lidar\n",
    "\n",
    "    # CODING: Calculate the angle between the lidar front and the x-axis\n",
    "    # This angle is the \"direction\" the lidar is facing\n",
    "    # Or rather, the center of the scanning cone\n",
    "    # Recall that an easy means of calculating angles is arctan2 - (check numpy docs)\n",
    "    angle = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "    # Generate angles for the rays using the angle and the number of rays\n",
    "    angles = np.linspace(angle - np.pi / 4, angle + np.pi / 4, num_rays)\n",
    "\n",
    "    intersections = []\n",
    "\n",
    "    # We will test each ray individually\n",
    "    for a in angles:\n",
    "        # CODING: First generate the ray endpoint\n",
    "        ray_end_x = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "        ray_end_y = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "        # We are going to find the closest intersection point with walls and circles,\n",
    "        # You will use these for that logic later on\n",
    "        closest_intersection = None\n",
    "        closest_distance = float('inf')\n",
    "\n",
    "        # We will write functions for checking line and circle intersections in a Check walls for intersection\n",
    "        for wall in room['walls']:\n",
    "            # CODING: Using the given lidar.line_intersection function, check for intersections\n",
    "            # and check if that intersection is closer than the current closest intersection\n",
    "            # if it is, update the closest intersection and distance\n",
    "            # The line_intersection function takes two points of the ray and two points of the wall\n",
    "            # The points are tuples of (x, y) coordinates, of which we have individual points for the lidar and ray end\n",
    "            # The wall is from the room objects we made earlier\n",
    "            intersection = None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "        # CODING: Now do the same for circles\n",
    "        # Bear in mind that it is unlikely for a ray to intersect with a circle just once\n",
    "        for circle in room['circles']:\n",
    "            intersection = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "\n",
    "        if closest_intersection:\n",
    "            intersections.append(closest_intersection)\n",
    "\n",
    "    return intersections\n",
    "# Cast lidar rays from the robot's front and check for intersections with walls and circles\n",
    "robot, lidar = plotting_utils.create_robot(62,20,140, size=4)\n",
    "lidar_intersections = cast_lidar_rays(lidar, room, num_rays=25)\n",
    "# Print the lidar intersections\n",
    "print(\"Lidar Intersections:\")\n",
    "for i, intersection in enumerate(lidar_intersections):\n",
    "    print(f\"  {intersection}\")\n",
    "    if i >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7ded2",
   "metadata": {},
   "source": [
    "If all went well, you should see this: (Double click this box to see it printed right ^.^)\n",
    "\n",
    "Lidar Intersections:\n",
    "  (58.472909877623195, 40.0)\n",
    "  (57.25130500680461, 40.0)\n",
    "  (56.00480194880258, 40.0)\n",
    "  (54.721540157960916, 40.0)\n",
    "  (53.388292490705574, 40.0)\n",
    "  (51.989879091978516, 40.0)\n",
    "  (50.508410909484226, 40.0)\n",
    "  (48.92227739903094, 40.0)\n",
    "  (48.0, 38.86425855653695)\n",
    "  (48.0, 36.663253524727864)\n",
    "  (46.89545935646254, 36.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plotting_utils.generate_map(room,size=40)\n",
    "\n",
    "# Generate some interesting lidar views by changing the robot's position and orientation\n",
    "# This, of course, if you managed to calculate the lidar intersections correctly\n",
    "robot, lidar = plotting_utils.create_robot(62,20,140, size=4) # These are the ones you calculated before\n",
    "lidar_intersections = cast_lidar_rays(lidar, room, num_rays=25)\n",
    "\n",
    "# visualize them here\n",
    "plotting_utils.plot_lidar_intersections(ax, robot, lidar, lidar_intersections)\n",
    "# Show the plot with lidar intersections\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77c41d",
   "metadata": {},
   "source": [
    "# \"Task\" 2\n",
    "\n",
    "(No coding here, just a playground -- **DO** play though, it gives perspective)\n",
    "\n",
    "So, now that we have seen what a single lidar scan ought to look like, let's get it moving and start messing it up a bit. The ``BotSimulation`` class is how we take a series a wheel velocities, translate them into global coordinates, and simulate lidar scans for each such global postion along the robot's path. \n",
    "\n",
    "Next, this class will allow us to simulate the [odometry](https://en.wikipedia.org/wiki/Odometry) noise that will cause the robot to believe it somewhere different than it.\n",
    "\n",
    "Finally, we can simulate lidar noise (kindof) which is normal in such scans, as lasers do not bounce very well off all surfaces. \n",
    "\n",
    "Note that when the robot moves, the lidar scans are coming from the **real** location, not the *perceived* one, but the scans are in the robot's **local** coordinates, so when simulating this, the scans show that the robot did not see the walls in the correct location. \n",
    "\n",
    "Play with the noise and scans here to see how much the different types of noise affect the quality of the percieved map.\n",
    "\n",
    "We will freeze the values for the main assignment, but this will help get an idea of why we need SLAM (**S**imultaneous **L**ocalisation **a**nd **M**apping). \n",
    "\n",
    "*(hint: the robot is the wrong place and map is wrong ^.- get it?)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = BotSimulation(cast_lidar_rays, robot_size=4, odometry_noise=0.05, lidar_noise=1, velocities=lidar_utils.wheel_velocities, num_scans = 25)\n",
    "# Run the simulation to get all data\n",
    "anim = simulation.animate_simulation_and_noise(simulation.all_data)\n",
    "# Save the animation as a GIF\n",
    "anim.save('noisy_robot_simulation.gif', fps=12)\n",
    "# Show the maps and animation\n",
    "plot_predicted_map(simulation)\n",
    "display(Image(url=f'noisy_robot_simulation.gif?{int(time.time())}', format='png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1cfee",
   "metadata": {},
   "source": [
    "# Tasks 3\n",
    "\n",
    "Now comes the **rough** stuff. \n",
    "\n",
    "We have provided the majority of the logic for ICP and Pose Graph Estimation. This process attempts to find correlations that are far away in the total robot path that can draw the noisy map recreation closer together (ICP), and thus bring the back to somewhere stable (PGE). \n",
    "\n",
    "This stabilization is a bit of a pytorch hack wherein we allow autograd to compute the gradients between the edges and nodes of the pose graph (rather than hessians in the usual process.....ew). \n",
    "\n",
    "The pose graph is a series of graph nodes wherein the location and orientation (global x, y, and theta) are stored in nodes, and the indicies and relative transformations (local dx, dy, and dtheta) between two nodes are stored as edges which serve as constraints in an optimization process. \n",
    "\n",
    "In effect, the odometry data is doodoo, but still a pretty good guess that stops the path from spiralling out of control. On the other hand, the lidar alignments are *\"accurate\"* (as long as you did it well), but far from where the noisy path currently is. \n",
    "\n",
    "The goal of the optimizer is the drag the path to somewhere that minimizes the the total \"tension\" on the edges by pulling the nodes to places that balance the odometry data and the lidar data. This will undo the noise form both odometry and lidar as long as the numerous hyperparameters are adjusted well.\n",
    "\n",
    "This code is pretty complex and tough to learn quickly (take it from me ftlog), so the task is to find the missing code and fill in the gaps. If I did my job right, the rest is still interesting to read and learn from. \n",
    "\n",
    "Take a look, finish the code, and proceed to task to 3.5 if you think you did it. Those blocks can help you test things. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507dd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseGraph:\n",
    "    def __init__(self, simulation, lidar_tolerance, training_steps):\n",
    "        self.pi = torch.tensor(np.pi, dtype=torch.float32)\n",
    "        self.simulation = simulation\n",
    "        # Initialize nodes based on the simulation path\n",
    "        # These nodes are the poses of the robot at each step in the simulation\n",
    "        # They also happen to be the very thing we will be optimizing\n",
    "        self.nodes = self.set_nodes()\n",
    "        self.all_data = self.simulation.run_simulation()  # Get all data from the simulation\n",
    "        self.edges = []\n",
    "        self.weights = []\n",
    "        self.lidar_tolerance = lidar_tolerance\n",
    "        self.training_steps = training_steps\n",
    "        self.lidar_information_factor = 5.0\n",
    "        self.odometry_information = 20.0\n",
    "\n",
    "    def compute_relative_transform(self, pose1, pose2):\n",
    "    # This function is used to find the local transformation from pose1 to pose2\n",
    "    # These are needed for Pose Graph Estimation\n",
    "        xi, yi, ti = pose1\n",
    "        xj, yj, tj = pose2\n",
    "\n",
    "        # Global difference\n",
    "        dx = xj - xi\n",
    "        dy = yj - yi\n",
    "        # Relative orientation\n",
    "        dtheta = torch.remainder(tj - ti + self.pi, 2 * self.pi) - self.pi # Rotation is normalized to the range [-pi, pi]\n",
    "\n",
    "        # To get the translation in pose1's local frame, we must\n",
    "        # rotate the global vector (dx, dy) by -ti.\n",
    "        # The rotation matrix R for -ti is:\n",
    "        #  [[cos(-ti), -sin(-ti)],\n",
    "        #   [sin(-ti),  cos(-ti)]]\n",
    "        # which simplifies to:\n",
    "        #  [[cos(ti),  sin(ti)],\n",
    "        #   [-sin(ti), cos(ti)]]\n",
    "\n",
    "        cos_ti = torch.cos(ti)\n",
    "        sin_ti = torch.sin(ti)\n",
    "\n",
    "        # CODING: Calculate the local coordinates of the translation\n",
    "        # Remember that the new location will come from R * [[dx], [dy]] (think matrix multiplicaiton)\n",
    "        local_dx = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "        local_dy = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "        return torch.stack([local_dx, local_dy, dtheta])\n",
    "\n",
    "    def set_nodes(self):\n",
    "        # This function sets the nodes based on the simulation path\n",
    "        nodes = []\n",
    "        for step in self.simulation.noise_path:\n",
    "            x, y, orientation = step\n",
    "            orientation = np.deg2rad(orientation)  # Convert orientation to radians\n",
    "            nodes.append((x, y, orientation))\n",
    "        return torch.tensor(nodes, dtype=torch.float32)  # Convert to tensor for consistency\n",
    "\n",
    "    def batch_trimmed_icp(self, source_points, target_points, initial_guess = None, max_iterations=1000, tolerance=1e-6):\n",
    "        source_points_raw = source_points.clone().to(dtype=torch.float32)\n",
    "        target_points = target_points.clone().to(dtype=torch.float32)\n",
    "        B = source_points.shape[0] # Batch size\n",
    "\n",
    "        if initial_guess is None:\n",
    "            transformations = torch.eye(3, device=source_points.device).unsqueeze(0).repeat(B, 1, 1)\n",
    "        else:\n",
    "            transformations = initial_guess.clone().to(dtype=torch.float32)\n",
    "\n",
    "        current_source_points = torch.einsum('bij,bnj->bni', transformations[:, :2, :2], source_points_raw) + transformations[:, :2, 2].unsqueeze(1)\n",
    "\n",
    "        converged = torch.zeros(B, dtype=torch.bool, device=source_points.device)\n",
    "\n",
    "        for _ in range(max_iterations): # ITERATIVE\n",
    "            if converged.all():\n",
    "                break\n",
    "\n",
    "            # Step 1: Find closest points using the current transformed points\n",
    "            # This means, for each source point cloud,\n",
    "            # we find the closest point in it's matched target point cloud\n",
    "            # (which happen to all be the same point cloud).\n",
    "            distances = torch.cdist(current_source_points, target_points) # Compute pairwise distances\n",
    "            # CODING: How do we get the indices of the closest points?\n",
    "            # Recall that the distances tensor is of shape (B, N_rays_source, N_rays_target)\n",
    "            closest_indices = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # Gather is a nasty function to wrap your head around, so we provide it. Check out how it works\n",
    "            closest_points = torch.gather(target_points, 1, closest_indices.unsqueeze(-1).repeat(1, 1, 2))\n",
    "\n",
    "            # Step 2: Trim points based on distance\n",
    "            trim_ratio = 0.6\n",
    "            num_keep = int(trim_ratio * current_source_points.shape[1])\n",
    "            dist_to_closest, _ = distances.min(dim=2)\n",
    "            sorted_indices = torch.argsort(dist_to_closest, dim=1)\n",
    "            keep_indices = sorted_indices[:, :num_keep]\n",
    "\n",
    "            inlier_source = torch.gather(current_source_points, 1, keep_indices.unsqueeze(-1).repeat(1, 1, 2))\n",
    "            inlier_target = torch.gather(closest_points, 1, keep_indices.unsqueeze(-1).repeat(1, 1, 2))\n",
    "\n",
    "            # Step 3: Compute centroids of the INLIERS\n",
    "            # CODING: How do we compute the centroids of the inliers?\n",
    "            # Hint: We want the average location over every ray in each batch\n",
    "            source_centroids = None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            target_centroids = None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "            # Step 4: Compute SVD on centered INLIERS\n",
    "            source_centered = inlier_source - source_centroids\n",
    "            target_centered = inlier_target - target_centroids\n",
    "            H = torch.einsum('bni,bnj->bij', source_centered, target_centered)\n",
    "\n",
    "            # Add a small epsilon for stability\n",
    "            H += torch.eye(2, device=H.device).unsqueeze(0) * 1e-9\n",
    "            # CODING how do we get U and Vt\n",
    "            U, _, Vt = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "            # Step 5: Compute incremental rotation and translation\n",
    "            # We want these to be incremental because we are moving the point cloud iteratively\n",
    "            # We apply the iterative rotation and translation to the current source points\n",
    "            # At the end, we will calcualte the final transformation from the original\n",
    "            # source points to those we iteratively align\n",
    "\n",
    "            # CODING: To get the rotation matrix R, we need to use the formula\n",
    "            # R = V * U^T\n",
    "            # Note: We have batches of matrices, does torch have a way to handle this?\n",
    "            # Note also: we have Vt and U, not V and U\n",
    "            R_iter = None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # Handle reflections\n",
    "            R_iter = batch_handle_reflections(R_iter, Vt, U)\n",
    "\n",
    "            t_iter = target_centroids.squeeze(1) - torch.einsum('bij,bj->bi', R_iter, source_centroids.squeeze(1))\n",
    "\n",
    "            # Step 6.-1: We only want to update the points that are not converged yet\n",
    "            not_converged_mask = ~converged\n",
    "            points_to_update = current_source_points[not_converged_mask]\n",
    "            R_inc = R_iter[not_converged_mask]\n",
    "            t_inc = t_iter[not_converged_mask]\n",
    "\n",
    "            # Step 6: WE are going to use the learned translation and rotation to update where the points \"are\"\n",
    "            # This is in applied to the original source points, not the inliers\n",
    "            centroid_of_points_to_update = torch.mean(points_to_update, dim=1, keepdim=True)\n",
    "\n",
    "            # Step 6.1: Center the points\n",
    "            centered_points = points_to_update - centroid_of_points_to_update\n",
    "\n",
    "            # Step 6.2: Apply incremental rotation\n",
    "            rotated_points = torch.einsum('bij,bnj->bni', R_inc, centered_points)\n",
    "\n",
    "            # Step 6.3: Add the centroid back and then apply the incremental translation\n",
    "            new_points = rotated_points + centroid_of_points_to_update + t_inc.unsqueeze(1)\n",
    "\n",
    "            # Step 6.4: Update the points for the next iteration\n",
    "            current_source_points[not_converged_mask] = new_points\n",
    "\n",
    "            # Step 7: Check for convergence\n",
    "            alignment_errors = torch.mean(torch.norm(current_source_points - closest_points, dim=2), dim=1)\n",
    "            newly_converged = (alignment_errors < tolerance) & (~converged)\n",
    "            converged = converged | newly_converged\n",
    "\n",
    "        # Step 8: Final Transformation Calculation\n",
    "        # Now that the points are aligned, we find the single transformation that\n",
    "        # maps the ORIGINAL raw points to the FINAL aligned points.\n",
    "\n",
    "        # Step 8.1: Centroids of original and final points\n",
    "        source_raw_centroids = torch.mean(source_points_raw, dim=1, keepdim=True)\n",
    "        final_points_centroids = torch.mean(current_source_points, dim=1, keepdim=True)\n",
    "\n",
    "        # Step 8.2: Centered points\n",
    "        source_raw_centered = source_points_raw - source_raw_centroids\n",
    "        final_points_centered = current_source_points - final_points_centroids\n",
    "\n",
    "        # Step 8.3: Final rotation\n",
    "        H_final = torch.einsum('bni,bnj->bij', source_raw_centered, final_points_centered)\n",
    "        # CODING how do we get U and Vt\n",
    "        U_final, _, Vt_final = None, None, None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "        # Coding: Another Rotation matrix, remember how to make this?\n",
    "        R_final = None# <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "        # Handle reflections in the final matrix\n",
    "        R_final = batch_handle_reflections(R_final, U_final, Vt_final)\n",
    "\n",
    "        # Step 8.4: Final translation\n",
    "        t_final = final_points_centroids.squeeze(1) - torch.einsum('bij,bj->bi', R_final, source_raw_centroids.squeeze(1))\n",
    "\n",
    "        # Step 8.5: Update the final transformation matrix\n",
    "        transformations[:, :2, :2] = R_final\n",
    "        transformations[:, :2, 2] = t_final\n",
    "\n",
    "        # Step 9: We now calculate the error against the closest points in the target set,\n",
    "        # which gives a true measure of alignment quality.\n",
    "\n",
    "        # Step 9.1: Find the final set of closest points from the target cloud\n",
    "        # We use closest points because the scans may have a non correlated ordering\n",
    "        final_distances = torch.cdist(current_source_points, target_points)\n",
    "        final_closest_points = torch.gather(target_points, 1, torch.argmin(final_distances, dim=2).unsqueeze(-1).repeat(1, 1, 2))\n",
    "\n",
    "        # Step 9.2: Calculate the mean distance to these closest points\n",
    "        final_alignment_errors = torch.mean(torch.norm(current_source_points - final_closest_points, dim=2), dim=1)\n",
    "\n",
    "        final_converged_mask = final_alignment_errors < self.lidar_tolerance\n",
    "\n",
    "        return current_source_points, transformations, final_alignment_errors, final_converged_mask\n",
    "\n",
    "    def extract_relative_pose_from_transformation(self, transformation):\n",
    "        \"\"\"\n",
    "        Extracts the relative pose (dx, dy, dtheta) from a transformation matrix.\n",
    "        The transformation matrix is expected to be in the form:\n",
    "        [[cos(theta), -sin(theta), dx],\n",
    "         [sin(theta),  cos(theta), dy],\n",
    "         [0,           0,          1]]\n",
    "        \"\"\"\n",
    "        dx = transformation[0, 2]\n",
    "        dy = transformation[1, 2]\n",
    "        dtheta = torch.atan2(transformation[1, 0], transformation[0, 0])  # Extract rotation angle\n",
    "        return torch.tensor([dx, dy, dtheta], dtype=torch.float32)\n",
    "\n",
    "    def lidar_edges(self):\n",
    "        # This function creates edges based on the lidar data\n",
    "        # It is quite complex so we provide it, feel free to read through (I commented good)\n",
    "        lidar_data = torch.tensor([data[5] for data in self.all_data], dtype=torch.float32)\n",
    "\n",
    "        loops_count = 0 # the number of loop closures found\n",
    "        timestep_limit = 25 # the minimum number of timesteps between two scans to find closures\n",
    "        spatial_distance_threshold = 10.0 # the minimum distance between two poses to consider them for loop closure\n",
    "        cross_check_threshold = 0.25 # Max allowed difference between forward and reverse transforms\n",
    "\n",
    "        pbar = tqdm(range(len(self.all_data)), desc=\"Finding Loop Closures\")\n",
    "\n",
    "        for i in pbar:\n",
    "            # We first need to select valid candidates so the code can run fast\n",
    "            # and so that the alignments are not redundant with the odometry edges\n",
    "            if lidar_data[i].nelement() == 0:\n",
    "                continue\n",
    "            start_index = i + timestep_limit\n",
    "            if start_index >= len(self.all_data):\n",
    "                continue\n",
    "            pose_i_xy = self.nodes[i, :2]\n",
    "            possible_indices = torch.arange(start_index, len(self.all_data))\n",
    "            if len(possible_indices) == 0:\n",
    "                continue\n",
    "            candidate_poses_xy = self.nodes[possible_indices, :2]\n",
    "            distances = torch.norm(candidate_poses_xy - pose_i_xy, dim=1)\n",
    "            close_enough_mask = distances < spatial_distance_threshold\n",
    "            candidate_indices = possible_indices[close_enough_mask]\n",
    "            if len(candidate_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            # --- BATCHED FORWARD ICP (j -> i) ---\n",
    "            # First we have to prepare the sets of scans\n",
    "            # WE compare every candidate j with a series of copies of targets i\n",
    "            source_scans = lidar_data[candidate_indices]\n",
    "            target_scan_batch = lidar_data[i].unsqueeze(0).repeat(len(candidate_indices), 1, 1)\n",
    "\n",
    "            # We feed the batch icp an intital guess based on the odometry data\n",
    "            # as long as the noise is low enough, this is a good initial guess\n",
    "            pose_i = self.nodes[i]\n",
    "            poses_j = self.nodes[candidate_indices]\n",
    "            T_i = torch.eye(3); T_i[0,0]=T_i[1,1]=torch.cos(pose_i[2]); T_i[0,1]=-torch.sin(pose_i[2]); T_i[1,0]=torch.sin(pose_i[2]); T_i[0,2]=pose_i[0]; T_i[1,2]=pose_i[1]\n",
    "            T_j = torch.eye(3).unsqueeze(0).repeat(len(poses_j),1,1); cos_tj,sin_tj=torch.cos(poses_j[:,2]),torch.sin(poses_j[:,2]); T_j[:,0,0]=T_j[:,1,1]=cos_tj; T_j[:,0,1]=-sin_tj; T_j[:,1,0]=sin_tj; T_j[:,0,2]=poses_j[:,0]; T_j[:,1,2]=poses_j[:,1]\n",
    "            T_i_world_to_local = torch.linalg.inv(T_i)\n",
    "            T_j_local_to_world = T_j\n",
    "            init_transform_forward = torch.bmm(T_i_world_to_local.unsqueeze(0).repeat(len(poses_j), 1, 1), T_j_local_to_world)\n",
    "\n",
    "            # Pperform ICP on the batch of scans\n",
    "            aligned_points, forward_transforms, alignment_errors, converged = self.batch_trimmed_icp(\n",
    "                source_scans, target_scan_batch,\n",
    "                initial_guess=init_transform_forward.to(dtype=torch.float32), tolerance=self.lidar_tolerance / 10, max_iterations=100\n",
    "            )\n",
    "            # We will only select the good alignments\n",
    "            final_distances = torch.cdist(aligned_points, target_scan_batch)\n",
    "            inlier_mask = final_distances.min(dim=2).values < self.lidar_tolerance\n",
    "            inlier_ratios = inlier_mask.float().mean(dim=1)\n",
    "\n",
    "            # Just to be sure, we will also run the \"good\" alignments the other direction\n",
    "            # This is a cross-check to ensure the match is stable and not ambiguous\n",
    "            for idx, j in enumerate(candidate_indices):\n",
    "                if converged[idx] and alignment_errors[idx] < self.lidar_tolerance and inlier_ratios[idx] > 0.5 :\n",
    "                    # This pair looks good. Now, perform the reverse check (i -> j).\n",
    "                    # We do this one by one as it's conditional.\n",
    "                    source_scan_reverse = lidar_data[i].unsqueeze(0)\n",
    "                    target_scan_reverse = lidar_data[j].unsqueeze(0)\n",
    "\n",
    "                    # Initial guess for the reverse transform is inv(T_i) * T_j\n",
    "                    initial_guess = torch.bmm(torch.linalg.inv(T_i.unsqueeze(0)), T_j[idx].unsqueeze(0))\n",
    "\n",
    "                    _, reverse_transform, _, _ = self.batch_trimmed_icp(\n",
    "                        source_scan_reverse, target_scan_reverse,\n",
    "                        initial_guess=initial_guess.to(dtype=torch.float32), tolerance=self.lidar_tolerance / 10, max_iterations=100\n",
    "                    )\n",
    "\n",
    "                    # The forward transform maps j -> i.\n",
    "                    # The reverse transform maps i -> j.\n",
    "                    # So, the inverse of the reverse transform should also map j -> i.\n",
    "                    inv_reverse_transform = torch.linalg.inv(reverse_transform.squeeze(0))\n",
    "                    forward_transform = forward_transforms[idx]\n",
    "\n",
    "                    # Compare the two. A small difference indicates a stable, non-ambiguous match.\n",
    "                    transform_diff = torch.norm(forward_transform - inv_reverse_transform)\n",
    "                    if transform_diff < cross_check_threshold:\n",
    "                        # This is a high-confidence match. Add the edge.\n",
    "                        relative_transform = self.extract_relative_pose_from_transformation(forward_transform)\n",
    "                        information = (self.lidar_information_factor * inlier_ratios[idx]) / (alignment_errors[idx] + 1e-6)\n",
    "                        self.add_edge(i, j, relative_transform, information)\n",
    "                        loops_count += 1\n",
    "\n",
    "            pbar.set_description(f\"Loops: {loops_count}\")\n",
    "        self.loop_count = loops_count\n",
    "\n",
    "    def odometry_edges(self):\n",
    "        # Create edges based on the odometry data\n",
    "        for i in range(len(self.nodes) - 1):\n",
    "            pose1 = self.nodes[i]\n",
    "            pose2 = self.nodes[i + 1]\n",
    "            relative_transform = self.compute_relative_transform(pose1, pose2)\n",
    "            self.add_edge(i, i + 1, relative_transform, self.odometry_information)  # Assign a constant weight for odometry edges\n",
    "\n",
    "    def add_edge(self, i,j, transformation, weight, lidar = False):\n",
    "        self.edges.append((i, j, transformation, weight))\n",
    "\n",
    "    def optimize(self):\n",
    "        # This optimizer is vectorized well, but we left some stuff out for you to implement\n",
    "        # The optimization will be done using PyTorch's autograd and Adam optimizer\n",
    "        # The nodes are the poses of the robot at each step in the simulation\n",
    "        optimized_nodes = self.nodes.clone().requires_grad_(True)\n",
    "        optimizer = torch.optim.Adam([optimized_nodes], lr=0.00005)\n",
    "\n",
    "        # Pre-process edges for vectorized computation\n",
    "        edge_indices_i = torch.tensor([e[0] for e in self.edges], dtype=torch.long)\n",
    "        edge_indices_j = torch.tensor([e[1] for e in self.edges], dtype=torch.long)\n",
    "\n",
    "        target_translations = torch.stack([e[2][:2] for e in self.edges]).to(optimized_nodes.device)\n",
    "        target_dtheta = torch.stack([e[2][2] for e in self.edges]).to(optimized_nodes.device)\n",
    "        # the weights have been added as elements of the edges\n",
    "        # We will use these to weight the loss function\n",
    "        weights = torch.tensor([e[3] for e in self.edges], dtype=torch.float32, device=optimized_nodes.device).unsqueeze(1)\n",
    "\n",
    "        # Here is where we get the ground truth path from the simulation\n",
    "        # This is the path we want to optimize towards\n",
    "        true_path_list = [list(p) for p in self.simulation.path]\n",
    "        true_path_rad = torch.tensor(true_path_list, dtype=torch.float32, device=optimized_nodes.device)\n",
    "        true_path_rad[:, 2] = torch.deg2rad(true_path_rad[:, 2])\n",
    "        # wWeights are the same for each\n",
    "        # Is this really optimal? Let me know\n",
    "        translation_weight = 1.0\n",
    "        rotation_weight = 1.0\n",
    "\n",
    "        pbar = tqdm(range(self.training_steps), desc=\"Optimizing Pose Graph\")\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            poses_i = optimized_nodes[edge_indices_i]\n",
    "            poses_j = optimized_nodes[edge_indices_j]\n",
    "\n",
    "            xi, yi, ti = poses_i.T\n",
    "            xj, yj, tj = poses_j.T\n",
    "\n",
    "            # Predicted GLOBAL difference (this is what we want to optimize)\n",
    "            predicted_global_dx = xj - xi\n",
    "            predicted_global_dy = yj - yi\n",
    "            predicted_global_vec = torch.stack([predicted_global_dx, predicted_global_dy], dim=1)\n",
    "\n",
    "            # Target local difference (from odometry/ICP measurements)\n",
    "            target_local_dx = target_translations[:, 0]\n",
    "            target_local_dy = target_translations[:, 1]\n",
    "\n",
    "            # Rotate the target local difference into the global frame using the CURRENT pose `ti`\n",
    "            cos_ti = torch.cos(ti)\n",
    "            sin_ti = torch.sin(ti)\n",
    "\n",
    "            # This is the forward rotation matrix for `ti`\n",
    "            # It transforms a vector from the local frame of pose `i` to the global frame.\n",
    "            target_global_dx = target_local_dx * cos_ti - target_local_dy * sin_ti\n",
    "            target_global_dy = target_local_dx * sin_ti + target_local_dy * cos_ti\n",
    "            target_global_vec = torch.stack([target_global_dx, target_global_dy], dim=1)\n",
    "\n",
    "            # Now, the translation error is a direct comparison of two global vectors.\n",
    "            # The gradient from this error will only flow to the positions (x,y), not the angle `ti`.\n",
    "            error_trans = torch.nn.functional.mse_loss(\n",
    "                predicted_global_vec,\n",
    "                target_global_vec,\n",
    "                reduction='none'\n",
    "            ).mean(dim=1, keepdim=True)\n",
    "\n",
    "            # Now we optimize the angles\n",
    "            # The rotation error is computed in the local frame of the target pose `j`\n",
    "            dtheta = tj - ti\n",
    "            predicted_rotations_vec = torch.stack([torch.cos(dtheta), torch.sin(dtheta)], dim=1)\n",
    "            target_rotations_vec = torch.stack([torch.cos(target_dtheta), torch.sin(target_dtheta)], dim=1)\n",
    "\n",
    "            error_rot = torch.nn.functional.mse_loss(\n",
    "                predicted_rotations_vec,\n",
    "                target_rotations_vec,\n",
    "                reduction='none'\n",
    "            ).mean(dim=1, keepdim=True)\n",
    "\n",
    "            # --- Combine errors with weights (Unchanged) ---\n",
    "            total_loss = torch.mean(\n",
    "                (translation_weight * error_trans + rotation_weight * error_rot) * weights\n",
    "            )\n",
    "            # CODING: Something obvious is missing here.\n",
    "            # I think it's two lines of code that you need to add\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "            # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # CODING : We want to ensure the first node is fixed\n",
    "                # This is because the first node is the origin of the pose graph\n",
    "                # How can we set it still?\n",
    "                optimized_nodes[0] = None # <EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                # These lines just print your training progress\n",
    "                # The loss is DIRECTLY related to the optimization\n",
    "                # The pos and rot error are only INDIRECTLY related\n",
    "                # If they go down, it's ACTUALLY working\n",
    "                with torch.no_grad():\n",
    "                    pos_error = torch.norm(true_path_rad[:, :2] - optimized_nodes[:, :2], dim=1).mean()\n",
    "                    d_theta = true_path_rad[:, 2] - optimized_nodes[:, 2]\n",
    "                    rot_error = torch.atan2(torch.sin(d_theta), torch.cos(d_theta)).abs().mean()\n",
    "                pbar.set_description(f\"Loss: {total_loss.item():.4f}\")\n",
    "                pbar.set_postfix({\n",
    "                    \"Pos Err\": f'{pos_error.item():.4f}',\n",
    "                    \"Rot Err\": f'{rot_error.item():.4f}'\n",
    "                })\n",
    "\n",
    "        self.nodes = optimized_nodes.detach()\n",
    "def batch_handle_reflections(R, Vt, U):\n",
    "    # R Bx2x2\n",
    "    det_R = torch.det(R)\n",
    "    reflection_mask_final = det_R < 0\n",
    "    if reflection_mask_final.any():\n",
    "        Vt_clone = Vt.clone()\n",
    "        Vt_clone[reflection_mask_final, -1, :] *= -1\n",
    "        R[reflection_mask_final] = torch.bmm(Vt_clone[reflection_mask_final].transpose(-1, -2), U[reflection_mask_final].transpose(-1, -2))\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed61cd",
   "metadata": {},
   "source": [
    "# Task 3.5\n",
    "Here are the blocks to actually test the training and visualise if it worked. We provide a stable seed that is garunteed to work with all the hyperparameters locked in AND the correct math in the other blocks. This means you can run them it to check your math. ^.^\n",
    "\n",
    "The task is to optimize the Pose Graph. If you fixed all the missing code well this should be no issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a31d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(69)  # Set a random seed for reproducibility\n",
    "simulation = BotSimulation(cast_lidar_rays, robot_size=4, odometry_noise=0.05, lidar_noise=0.0,\n",
    "                           velocities=lidar_utils.wheel_velocities, num_scans = 100)\n",
    "prev_edges = None # this line is honestly only here to help not recacluate lidar if it helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "pose_graph = PoseGraph(simulation, 1.5, 400000)\n",
    "pose_graph.odometry_edges()\n",
    "if prev_edges:\n",
    "    pose_graph.edges = prev_edges  # Use the previous edges if they exist\n",
    "else:\n",
    "    pose_graph.lidar_edges()\n",
    "prev_edges = pose_graph.edges.copy()\n",
    "pose_graph.optimize()\n",
    "adjusted_simulation = deepcopy(simulation)\n",
    "adjusted_simulation.noise_path = pose_graph.nodes.detach().numpy().tolist()\n",
    "optimized_nodes_numpy = pose_graph.nodes.detach().numpy()\n",
    "optimized_nodes_numpy[:, 2] = np.rad2deg(optimized_nodes_numpy[:, 2])\n",
    "adjusted_simulation.noise_path = optimized_nodes_numpy.tolist()\n",
    "\n",
    "# 3. Re-run the simulation logic to generate a NEW `all_data` packet.\n",
    "#    This new packet will have robot poses (`pred_robot`) that follow the optimized path.\n",
    "adjusted_all_data = adjusted_simulation.run_simulation()\n",
    "\n",
    "# 4. Animate using the NEWLY generated data.\n",
    "adjusted_anim = adjusted_simulation.animate_simulation_and_noise(adjusted_all_data)\n",
    "\n",
    "# Save the adjusted animation as a GIF. This will now show the corrected path and map.\n",
    "adjusted_anim.save('adjusted_robot_simulation.gif', writer='pillow', fps=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1ea81",
   "metadata": {},
   "source": [
    "# Task \"4\"\n",
    "This isn't really a task, but if you have time, play with some of the hyperparameters of the training:\n",
    " - lr\n",
    " - odo noise\n",
    " - lidar noise\n",
    " - loops_count \n",
    " - timestep_limit \n",
    " - spatial_distance_threshold\n",
    " - cross_check_threshold \n",
    " - etc...\n",
    "\n",
    " If my hypothesis is correct, most of them will totally mess it up, but you may be able to find a better alignment. Let me know! In any case, seeing how each parameter affects this delicate process can sometimes lead to strong insights.\n",
    "\n",
    " Furthermore, if you have any feedback, I would love to know.\n",
    "\n",
    " Evenfurthermore, if you have some research ideas on how to do this better, **write them down**. Glad to inspire you, this field is neato and worth looking into. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f10fd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSBB19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
